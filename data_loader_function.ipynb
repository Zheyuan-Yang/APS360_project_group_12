{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "APS360.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-PKJfLv-6N5G"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchtext\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load data from csv file\n",
        "fields = ['news_article', 'news_category']\n",
        "\n",
        "train_data = pd.read_csv('/content/inshort_news_data-train.csv', header=0, encoding='ISO-8859-1', usecols=fields, skip_blank_lines=True)\n",
        "val_data = pd.read_csv('/content/inshort_news_data-val.csv', header=0, encoding='ISO-8859-1', usecols=fields, skip_blank_lines=True)\n",
        "test_data = pd.read_csv('/content/inshort_news_data-test.csv', header=0, encoding='ISO-8859-1', usecols=fields, skip_blank_lines=True)\n"
      ],
      "metadata": {
        "id": "DOZ2dAbG6-jP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Num training articles: ', len(train_data))\n",
        "print('Num validation articles: ', len(val_data))\n",
        "print('Num testing articles: ', len(test_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rsoRXNQ7D_X",
        "outputId": "913117ec-f5ef-44ec-9461-379b29234a79"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num training articles:  6380\n",
            "Num validation articles:  1560\n",
            "Num testing articles:  1742\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating training and testing data\n",
        "X_train = train_data['news_article']\n",
        "Y_train = train_data['news_category']\n",
        "\"\"\"\n",
        "Y_train = np.zeros((X_train.shape[0],1))\n",
        "for i in range((X_train.shape[0])):\n",
        "  for j in range(7):\n",
        "    if (train[j+1][i]==1):\n",
        "      Y_train[i]=j\n",
        "\"\"\"\n",
        "X_test = test_data['news_article']\n",
        "Y_test = test_data['news_category']\n",
        "\"\"\"\n",
        "Y_test = np.zeros((X_test.shape[0],1))\n",
        "for i in range((X_test.shape[0])):\n",
        "  for j in range(7):\n",
        "    if (test[j+1][i]==1):\n",
        "      Y_test[i]=j\n",
        "\"\"\"\n",
        "X_val = val_data['news_article']\n",
        "Y_val = val_data['news_category']\n",
        "\n",
        "print (X_train.shape, Y_train.shape, X_val.shape, Y_val.shape, X_test.shape, Y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3B9s52UAb9f",
        "outputId": "1647c09e-9641-4b2e-ed84-040de5b6bb9b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6380,) (6380,) (1560,) (1560,) (1742,) (1742,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(X_train.shape[0]):\n",
        "  X_train[i] = X_train[i].split()\n",
        "\n",
        "for j in range(X_val.shape[0]):\n",
        "  X_val[j] = X_val[j].split()\n",
        "\n",
        "for k in range(X_test.shape[0]):\n",
        "  X_test[k] = X_test[k].split()\n",
        "    \n",
        "Y_train = pd.get_dummies(Y_train).to_numpy()\n",
        "Y_val = pd.get_dummies(Y_val).to_numpy()\n",
        "Y_test = pd.get_dummies(Y_test).to_numpy()"
      ],
      "metadata": {
        "id": "uQawqWmUFEYl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to see what is the largest number of words in a article\n",
        "# np.unique(np.array([len(ix) for ix in X_train]) , return_counts=True)\n",
        "np.unique(np.array([len(ix) for ix in X_val]) , return_counts=True)\n",
        "# np.unique(np.array([len(ix) for ix in X_test]) , return_counts=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSmghLhXMdva",
        "outputId": "1a84e35d-b731-4538-cc62-19c003d88a79"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([43, 46, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60]),\n",
              " array([  1,   2,   2,   6,  11,  15,  22,  18,  39,  72,  90, 107, 159,\n",
              "        312, 704]))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# stopwords to eliminate useless words\n",
        "stopwords = []\n",
        "stop = open('/content/stopwords.txt', encoding=\"utf-8\")\n",
        "for line in stop:\n",
        "  stopwords.append(line.strip())\n",
        "stop.close()"
      ],
      "metadata": {
        "id": "zeV9SswhMHbG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# utilize Glove6B for embedding\n",
        "glove = torchtext.vocab.GloVe(name='6B', dim=50)"
      ],
      "metadata": {
        "id": "c-Lcvjv5LsKa"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filling the embedding matrix\n",
        "embedding_matrix_train = np.zeros((X_train.shape[0], 61, 50))\n",
        "embedding_matrix_val = np.zeros((X_val.shape[0], 61, 50))\n",
        "embedding_matrix_test = np.zeros((X_test.shape[0], 61, 50))\n",
        "\n",
        "for i in range(X_train.shape[0]):\n",
        "  for j in range(len(X_train[i])):\n",
        "    if not (X_train[i][j].lower() in stopwords):\n",
        "      embedding_matrix_train[i][j] = glove[X_train[i][j].lower()]\n",
        "\n",
        "for i in range(X_val.shape[0]):\n",
        "  for j in range(len(X_val[i])):\n",
        "    if not (X_val[i][j].lower() in stopwords):\n",
        "      embedding_matrix_val[i][j] = glove[X_val[i][j].lower()]\n",
        "\n",
        "for i in range(X_test.shape[0]):\n",
        "  for j in range(len(X_test[i])):\n",
        "    if not (X_test[i][j].lower() in stopwords):\n",
        "      embedding_matrix_test[i][j] = glove[X_test[i][j].lower()] "
      ],
      "metadata": {
        "id": "3SuHNbq7MiNg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_t = torch.from_numpy(embedding_matrix_train).to(torch.float32)\n",
        "Y_train_t = torch.from_numpy(Y_train).to(torch.float32)\n",
        "X_val_t = torch.from_numpy(embedding_matrix_val).to(torch.float32)\n",
        "Y_val_t = torch.from_numpy(Y_val).to(torch.float32)\n",
        "X_test_t = torch.from_numpy(embedding_matrix_test).to(torch.float32)\n",
        "Y_test_t = torch.from_numpy(Y_test).to(torch.float32)"
      ],
      "metadata": {
        "id": "-d2rVG59jG8j"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TensorDataset(X_train_t, Y_train_t)\n",
        "val_dataset = TensorDataset(X_val_t, Y_val_t)\n",
        "test_dataset = TensorDataset(X_test_t, Y_test_t)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=128)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=128)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=128)"
      ],
      "metadata": {
        "id": "dUUEO7KNjKww"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}